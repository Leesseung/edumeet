from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import re
import math
import os
from collections import Counter
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import httpx
from dotenv import load_dotenv
import logging

# KoNLPy ì„ íƒì  import (ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ëŒ€ë¹„)
try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    logger = logging.getLogger(__name__)
    logger.warning("KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê¸°ëŠ¥ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
    KONLPY_AVAILABLE = False
    Okt = None

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path="config.env")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(title="êµìœ¡ í…ìŠ¤íŠ¸ ìš”ì•½ API", version="1.0.0")

# ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def log_requests(request, call_next):
    print(f"ğŸŒ ìš”ì²­: {request.method} {request.url}")
    logger.info(f"ìš”ì²­: {request.method} {request.url}")
    
    try:
        response = await call_next(request)
        print(f"ğŸŒ ì‘ë‹µ: {response.status_code}")
        logger.info(f"ì‘ë‹µ: {response.status_code}")
        return response
    except Exception as e:
        print(f"ğŸŒ ë¯¸ë“¤ì›¨ì–´ ì˜¤ë¥˜: {e}")
        logger.error(f"ë¯¸ë“¤ì›¨ì–´ ì˜¤ë¥˜: {e}")
        raise

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ëª¨ë¸ ì´ˆê¸°í™”
embedding_model = None
okt = None

@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global embedding_model, okt
    try:
        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        
        # KoNLPy Okt ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
        if KONLPY_AVAILABLE:
            okt = Okt()
            logger.info("KoNLPy Okt ë¡œë“œ ì™„ë£Œ")
        else:
            logger.warning("KoNLPyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ëŠ¥ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë¹ ë¥´ê³  ê°€ë²¼ìš´ ëª¨ë¸)
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        logger.info("Sentence Transformer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜
class TextExtractionRequest(BaseModel):
    text: str
    extractKeywords: bool = True
    extractSentences: bool = True

class TextExtractionResponse(BaseModel):
    keywords: Optional[List[str]] = None
    keySentences: Optional[List[str]] = None

class LLMSummarizeRequest(BaseModel):
    text: str

class LLMSummarizeResponse(BaseModel):
    summary: str

class TextFilterRequest(BaseModel):
    text: str
    similarity_threshold: float = 0.3
    min_sentence_length: int = 20

class TextFilterResponse(BaseModel):
    filtered_text: str
    removed_sentences: int
    total_sentences: int
    similarity_scores: Optional[List[float]] = None
    okt_analysis: Optional[str] = None

# êµìœ¡ ë¶„ì•¼ ì¤‘ìš” í’ˆì‚¬ íƒœê·¸ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ì¤‘ì‹¬)
IMPORTANT_POS_TAGS = {
    'Noun',      # ëª…ì‚¬
    'Verb',      # ë™ì‚¬  
    'Adjective', # í˜•ìš©ì‚¬
    'VerbPrefix' # ë™ì‚¬ ì ‘ë‘ì‚¬
}

# êµ¬ì–´ì²´ í‘œí˜„ íŒ¨í„´ (ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì œê±°í•  íŒ¨í„´ë“¤)
COLLOQUIAL_PATTERNS = [
    r'\b(ìŒ|ì–´|ì•„|ì˜¤|ì™€|í—‰|ì–´ë¨¸|ì•„ì´ê³ )\b',  # ê°íƒ„ì‚¬
    r'\b(ê·¸ëƒ¥|ë­”ê°€|ë§‰|ì¢€|ì§„ì§œ|ì •ë§|ì™„ì „|ì—„ì²­|ë˜ê²Œ|ë„ˆë¬´)\s+',  # êµ¬ì–´ì²´ ë¶€ì‚¬
    r'(.{1,10}?)\1{2,}',  # ë°˜ë³µ í‘œí˜„
    r'\s+',  # ê³¼ë„í•œ ê³µë°±
]

# êµìœ¡ ê´€ë ¨ ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´ (ê°€ì¤‘ì¹˜ ë¶€ì—¬ìš©)
EDUCATION_KEYWORD_PATTERNS = [
    r'(ê°œë…|ì •ì˜|ì›ë¦¬|ë²•ì¹™|ì´ë¡ |ê³µì‹|ì •ë¦¬|ì¦ëª…)',
    r'(ì˜ˆì‹œ|ì‚¬ë¡€|ë¬¸ì œ|í•´ê²°|ë°©ë²•|ê³¼ì •|ë‹¨ê³„|ì ˆì°¨)',
    r'(ê²°ê³¼|ê²°ë¡ |ìš”ì•½|ì •ë¦¬|ì¤‘ìš”|í•µì‹¬|ì£¼ìš”)',
    r'(ê¸°ë³¸|ê¸°ì´ˆ|ì‘ìš©|í™œìš©|ì‹¤ìŠµ|ì—°ìŠµ|ë³µìŠµ)',
    r'(ì‹œí—˜|í‰ê°€|ê³¼ì œ|ìˆ™ì œ|í•™ìŠµ|ì´í•´|ì•”ê¸°|ê¸°ì–µ|ë¶„ì„|ë¹„êµ)'
]

def preprocess_education_text(text: str) -> str:
    """êµìœ¡ íŠ¹í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (êµ¬ì–´ì²´ â†’ ë¬¸ì–´ì²´) - ê°œì„ ëœ ë²„ì „"""
    processed_text = text
    
    # êµ¬ì–´ì²´ íŒ¨í„´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì •ë¦¬
    for pattern in COLLOQUIAL_PATTERNS:
        if pattern == r'\s+':  # ë§ˆì§€ë§‰ì— ê³µë°± ì •ë¦¬
            processed_text = re.sub(pattern, ' ', processed_text)
        else:
            processed_text = re.sub(pattern, '', processed_text)
    
    return processed_text.strip()

def analyze_morphology_with_okt(text: str) -> List[tuple]:
    """KoNLPy Oktë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„"""
    if not KONLPY_AVAILABLE or not okt:
        logger.warning("OKTê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹¨ì–´ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ê°„ë‹¨í•œ ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
        words = re.findall(r'[ê°€-í£]{2,}', text)
        return [(word, 'Noun') for word in words]
    
    try:
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        preprocessed_text = preprocess_education_text(text)
        
        # í˜•íƒœì†Œ ë¶„ì„ ë° í’ˆì‚¬ íƒœê¹…
        morphs_with_pos = okt.pos(preprocessed_text, stem=True)
        
        # ì¤‘ìš”í•œ í’ˆì‚¬ë§Œ í•„í„°ë§ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±)
        important_morphs = [
            (word, pos) for word, pos in morphs_with_pos
            if pos in IMPORTANT_POS_TAGS and len(word) > 1
        ]
        
        return important_morphs
        
    except Exception as e:
        logger.error(f"í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return []

def extract_keywords_with_okt(text: str, top_k: int = 15) -> List[str]:
    """OKT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (í’ˆì‚¬ íƒœê¹… í™œìš©)"""
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    
    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í˜•íƒœì†Œ ë¶„ì„
    all_morphs = analyze_morphology_with_okt(text)
    
    if not all_morphs:
        logger.warning("í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # ë‹¨ì–´ë³„ ë¹ˆë„ ê³„ì‚° (í’ˆì‚¬ë³„ ê°€ì¤‘ì¹˜ ì ìš©)
    word_freq = Counter()
    for word, pos in all_morphs:
        # í’ˆì‚¬ë³„ ê°€ì¤‘ì¹˜
        weight = 1.0
        if pos == 'Noun':
            weight = 1.5  # ëª…ì‚¬ ê°€ì¤‘ì¹˜
        elif pos == 'Verb':
            weight = 1.3  # ë™ì‚¬ ê°€ì¤‘ì¹˜
        elif pos == 'Adjective':
            weight = 1.2  # í˜•ìš©ì‚¬ ê°€ì¤‘ì¹˜
        
        # êµìœ¡ í‚¤ì›Œë“œ ì¶”ê°€ ê°€ì¤‘ì¹˜
        for pattern in EDUCATION_KEYWORD_PATTERNS:
            if re.search(pattern, word):
                weight *= 1.5
                break
        
        word_freq[word] += weight
    
    # ë¬¸ì„œ ë¹ˆë„ ê³„ì‚° (DF)
    doc_freq = Counter()
    for sentence in sentences:
        sentence_morphs = analyze_morphology_with_okt(sentence)
        sentence_words = set([word for word, _ in sentence_morphs])
        for word in sentence_words:
            doc_freq[word] += 1
    
    # TF-IDF ê³„ì‚°
    tfidf = {}
    total_docs = len(sentences)
    total_words = sum(word_freq.values())
    
    for word, freq in word_freq.items():
        tf = freq / total_words
        idf = math.log(total_docs / (doc_freq[word] if doc_freq[word] > 0 else 1))
        tfidf[word] = tf * idf
    
    # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
    sorted_keywords = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)
    return [word for word, _ in sorted_keywords[:top_k]]

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ë„ ìœ ì§€ (í˜¸í™˜ì„±)
def analyze_morphology(text: str) -> List[str]:
    """ê¸°ì¡´ í˜•íƒœì†Œ ë¶„ì„ (í˜¸í™˜ì„± ìœ ì§€)"""
    morphs_with_pos = analyze_morphology_with_okt(text)
    return [word for word, _ in morphs_with_pos]

def extract_keywords(text: str, top_k: int = 15) -> List[str]:
    """êµìœ¡ íŠ¹í™” TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ"""
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    all_words = analyze_morphology(text)
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚° (TF) - êµìœ¡ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©
    word_freq = Counter()
    for word in all_words:
        weight = 1.5 if word in EDUCATION_KEYWORDS else 1.0
        word_freq[word] += weight
    
    # ë¬¸ì„œ ë¹ˆë„ ê³„ì‚° (DF)
    doc_freq = Counter()
    for sentence in sentences:
        sentence_words = set(analyze_morphology(sentence))
        for word in sentence_words:
            doc_freq[word] += 1
    
    # TF-IDF ê³„ì‚°
    tfidf = {}
    total_docs = len(sentences)
    total_words = len(all_words)
    
    for word, freq in word_freq.items():
        tf = freq / total_words
        idf = math.log(total_docs / (doc_freq[word] if doc_freq[word] > 0 else 1))
        tfidf[word] = tf * idf
    
    # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
    sorted_keywords = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)
    return [word for word, _ in sorted_keywords[:top_k]]

def extract_key_sentences(text: str, top_k: int = 6) -> List[str]:
    """êµìœ¡ íŠ¹í™” í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (OKT ê¸°ë°˜)"""
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 15]
    keywords = extract_keywords_with_okt(text, 25)
    keyword_set = set(keywords)
    
    # êµìœ¡ íŠ¹í™” ë¬¸ì¥ ì ìˆ˜ ê³„ì‚°
    sentence_scores = []
    for index, sentence in enumerate(sentences):
        words = analyze_morphology(sentence)
        keyword_count = sum(1 for word in words if word in keyword_set)
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì ìˆ˜
        score = keyword_count / max(len(words), 1)
        
        # êµìœ¡ íŠ¹í™” ê°€ì¤‘ì¹˜
        if re.search(r'ì¤‘ìš”|í•µì‹¬|ê¸°ë³¸|ê°œë…|ì •ì˜|ì›ë¦¬', sentence):
            score *= 1.3
        if re.search(r'ì˜ˆë¥¼ ë“¤ì–´|ì˜ˆì‹œ|ì‚¬ë¡€|ì‹¤ìŠµ', sentence):
            score *= 1.2
        if re.search(r'ì •ë¦¬í•˜ë©´|ìš”ì•½í•˜ë©´|ê²°ë¡ |ë§ˆë¬´ë¦¬', sentence):
            score *= 1.4
        if re.search(r'ì‹œí—˜|í‰ê°€|ê³¼ì œ', sentence):
            score *= 1.2
        
        # ìœ„ì¹˜ ê°€ì¤‘ì¹˜ (ë„ì…ë¶€ì™€ ë§ˆë¬´ë¦¬ ë¶€ë¶„ ì¤‘ìš”)
        position = index / len(sentences)
        if position < 0.2 or position > 0.8:
            score *= 1.1
        
        sentence_scores.append({
            'sentence': sentence,
            'score': score,
            'length': len(sentence),
            'position': index
        })
    
    # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ë˜, ì ì ˆí•œ ê¸¸ì´ì˜ ë¬¸ì¥ë§Œ ì„ ë³„
    filtered_scores = [
        item for item in sentence_scores 
        if 25 < item['length'] < 200
    ]
    
    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ ì„ íƒ, ì›ë˜ ìˆœì„œë¡œ ì¬ì •ë ¬
    top_sentences = sorted(filtered_scores, key=lambda x: x['score'], reverse=True)[:top_k]
    top_sentences.sort(key=lambda x: x['position'])
    
    return [item['sentence'] for item in top_sentences]

def tokenize_sentences(text: str) -> List[str]:
    """ë¬¸ì¥ ë¶„í• """
    try:
        sentences = nltk.sent_tokenize(text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    except Exception as e:
        logger.warning(f"NLTK í† í°í™” ì‹¤íŒ¨, ê°„ë‹¨í•œ ë¶„í•  ì‚¬ìš©: {e}")
        return [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 10]

def filter_text_by_similarity_with_okt(text: str, similarity_threshold: float = 0.3, min_sentence_length: int = 20) -> Dict[str, Any]:
    """OKT ê¸°ë°˜ ê°œì„ ëœ ìœ ì‚¬ë„ í•„í„°ë§"""
    if not embedding_model or not okt:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ë¬¸ì¥ ë¶„í• 
    sentences = tokenize_sentences(text)
    sentences = [s for s in sentences if len(s) >= min_sentence_length]
    
    if len(sentences) < 2:
        return {
            "filtered_text": text,
            "removed_sentences": 0,
            "total_sentences": len(sentences),
            "okt_analysis": "ë¬¸ì¥ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ í•„í„°ë§í•˜ì§€ ì•ŠìŒ"
        }
    
    try:
        # ê° ë¬¸ì¥ì—ì„œ ì¤‘ìš”í•œ í˜•íƒœì†Œë§Œ ì¶”ì¶œí•˜ì—¬ ì„ë² ë”©ì— ì‚¬ìš©
        processed_sentences = []
        morphology_info = []
        
        for sentence in sentences:
            # OKTë¡œ í˜•íƒœì†Œ ë¶„ì„
            morphs = analyze_morphology_with_okt(sentence)
            important_words = [word for word, pos in morphs if len(word) > 1]
            
            # ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ë¡œ ë¬¸ì¥ ì¬êµ¬ì„±
            processed_sentence = ' '.join(important_words) if important_words else sentence
            processed_sentences.append(processed_sentence)
            morphology_info.append({
                "original": sentence,
                "morphs": morphs,
                "important_words": important_words
            })
        
        # ì²˜ë¦¬ëœ ë¬¸ì¥ë“¤ë¡œ ì„ë² ë”© ìƒì„±
        embeddings = embedding_model.encode(processed_sentences)
        
        # ì „ì²´ ì¤‘ìš” í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ì¤€ ì„ë² ë”© ìƒì„±
        all_important_words = []
        for info in morphology_info:
            all_important_words.extend(info["important_words"])
        
        if all_important_words:
            # í•µì‹¬ í‚¤ì›Œë“œë“¤ì˜ ì„ë² ë”©ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©
            core_keywords = extract_keywords_with_okt(text, 10)
            reference_text = ' '.join(core_keywords)
            reference_embedding = embedding_model.encode([reference_text])
        else:
            # í‰ê·  ì„ë² ë”©ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©
            reference_embedding = np.mean(embeddings, axis=0).reshape(1, -1)
        
        # ê° ë¬¸ì¥ê³¼ ê¸°ì¤€ì  ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(embeddings, reference_embedding).flatten()
        
        # ì„ê³„ê°’ ì´ìƒì˜ ë¬¸ì¥ë§Œ ì„ íƒ
        filtered_indices = [
            i for i, sim in enumerate(similarities) 
            if sim >= similarity_threshold
        ]
        
        # ìµœì†Œ ë¬¸ì¥ ìˆ˜ ë³´ì¥ (ë„ˆë¬´ ë§ì´ í•„í„°ë§ë˜ëŠ” ê²ƒ ë°©ì§€)
        min_keep = max(2, int(len(sentences) * 0.4))  # 40% ì´ìƒì€ ë³´ì¥
        if len(filtered_indices) < min_keep:
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ ë¬¸ì¥ë“¤ ë³´ì¥
            indices_with_sim = [(i, sim) for i, sim in enumerate(similarities)]
            indices_with_sim.sort(key=lambda x: x[1], reverse=True)
            filtered_indices = [i for i, _ in indices_with_sim[:min_keep]]
        
        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        filtered_indices.sort()
        filtered_sentences = [sentences[i] for i in filtered_indices]
        
        return {
            "filtered_text": " ".join(filtered_sentences),
            "removed_sentences": len(sentences) - len(filtered_sentences),
            "total_sentences": len(sentences),
            "similarity_scores": [float(sim) for sim in similarities],
            "okt_analysis": f"í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ, í•µì‹¬ í‚¤ì›Œë“œ: {core_keywords[:5] if 'core_keywords' in locals() else 'ì—†ìŒ'}"
        }
        
    except Exception as e:
        logger.error(f"OKT ê¸°ë°˜ ìœ ì‚¬ë„ í•„í„°ë§ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
        return filter_text_by_similarity(text, similarity_threshold, min_sentence_length)

def filter_text_by_similarity(text: str, similarity_threshold: float = 0.3, min_sentence_length: int = 20) -> Dict[str, Any]:
    """ìœ ì‚¬ë„ ê¸°ë°˜ í…ìŠ¤íŠ¸ í•„í„°ë§"""
    if not embedding_model:
        raise HTTPException(status_code=500, detail="ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ë¬¸ì¥ ë¶„í• 
    sentences = tokenize_sentences(text)
    sentences = [s for s in sentences if len(s) >= min_sentence_length]
    
    if len(sentences) < 2:
        return {
            "filtered_text": text,
            "removed_sentences": 0,
            "total_sentences": len(sentences)
        }
    
    try:
        # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
        embeddings = embedding_model.encode(sentences)
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚° (ê¸°ì¤€ì )
        mean_embedding = np.mean(embeddings, axis=0).reshape(1, -1)
        
        # ê° ë¬¸ì¥ê³¼ í‰ê·  ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(embeddings, mean_embedding).flatten()
        
        # ì„ê³„ê°’ ì´ìƒì˜ ë¬¸ì¥ë§Œ ì„ íƒ
        filtered_sentences = [
            sentences[i] for i, sim in enumerate(similarities) 
            if sim >= similarity_threshold
        ]
        
        # ìµœì†Œ ë¬¸ì¥ ìˆ˜ ë³´ì¥ (ë„ˆë¬´ ë§ì´ í•„í„°ë§ë˜ëŠ” ê²ƒ ë°©ì§€)
        if len(filtered_sentences) < max(2, len(sentences) * 0.3):
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ 30% ì´ìƒì€ ë³´ì¥
            min_keep = max(2, int(len(sentences) * 0.3))
            indices_with_sim = [(i, sim) for i, sim in enumerate(similarities)]
            indices_with_sim.sort(key=lambda x: x[1], reverse=True)
            filtered_sentences = [sentences[i] for i, _ in indices_with_sim[:min_keep]]
        
        return {
            "filtered_text": " ".join(filtered_sentences),
            "removed_sentences": len(sentences) - len(filtered_sentences),
            "total_sentences": len(sentences)
        }
        
    except Exception as e:
        logger.error(f"ìœ ì‚¬ë„ í•„í„°ë§ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
        return {
            "filtered_text": text,
            "removed_sentences": 0,
            "total_sentences": len(sentences)
        }

@app.post("/api/extract-key-sentences", response_model=TextExtractionResponse)
async def extract_key_sentences_api(request: TextExtractionRequest):
    """í‚¤ì›Œë“œ/ë¬¸ì¥ ì¶”ì¶œ API"""
    try:
        if not request.text or not request.text.strip():
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        result = {}
        
        if request.extractKeywords:
            result["keywords"] = extract_keywords_with_okt(request.text, 10)
        
        if request.extractSentences:
            result["keySentences"] = extract_key_sentences(request.text, 3)
        
        return TextExtractionResponse(**result)
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ/ë¬¸ì¥ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í‚¤ì›Œë“œ/ë¬¸ì¥ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@app.post("/api/llm-summarize", response_model=LLMSummarizeResponse)
async def llm_summarize_api(request: LLMSummarizeRequest):
    """LLM ìš”ì•½ API"""
    print("ğŸ”¥ LLM API í˜¸ì¶œë¨!")  # ê°•ì œ ì¶œë ¥
    logger.info("ğŸ”¥ LLM API ìš”ì²­ ë°›ìŒ")
    try:
        print(f"ğŸ”¥ ìš”ì²­ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(request.text) if request.text else 0}")
        logger.info(f"LLM API ìš”ì²­ ë°›ìŒ: {len(request.text) if request.text else 0}ì")
        
        if not request.text or not request.text.strip():
            raise HTTPException(status_code=400, detail="ìš”ì•½í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        openai_api_key = os.getenv("OPENAI_API_KEY")
        logger.info(f"API í‚¤ í™•ì¸: {bool(openai_api_key)}, ê¸¸ì´: {len(openai_api_key) if openai_api_key else 0}")
        
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ì‘ë‹µ
        if not openai_api_key or openai_api_key == "your_openai_api_key_here":
            summary = f"""ğŸ“ **ìš”ì•½ ê²°ê³¼**

í•µì‹¬ ë‚´ìš©: {request.text[:80]}...

ì´ ë¬¸ì„œëŠ” ì¤‘ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ì£¼ìš” ë…¼ì ë“¤ì´ ì²´ê³„ì ìœ¼ë¡œ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ìœ ìš©í•œ ì°¸ê³  ìë£Œë¡œ í™œìš©í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.

âš ï¸ *ì‹¤ì œ AI ìš”ì•½ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”*"""
            
            return LLMSummarizeResponse(summary=summary)

        # SSAFY GMS API ë˜ëŠ” OpenAI API í˜¸ì¶œ
        is_gms_api_key = openai_api_key.startswith('S13P')  # SSAFY GMS API í‚¤ íŒ¨í„´ ìˆ˜ì •
        api_url = ("https://gms.ssafy.io/gmsapi/api.openai.com/v1/chat/completions" 
                  if is_gms_api_key else "https://api.openai.com/v1/chat/completions")

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                api_url,
                headers={
                    "Authorization": f"Bearer {openai_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-4o-mini",
                    "messages": [
                        {
                            "role": "system",
                            "content": """ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ ìš”ì•½ AIì…ë‹ˆë‹¤. ì„ ìƒë‹˜ì˜ 1ì‹œê°„ ê°•ì˜ ë‚´ìš©ì„ í•™ìƒë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ìš”ì•½ í˜•ì‹:
ğŸ“š **ì£¼ìš” í•™ìŠµ ë‚´ìš©**
- í•µì‹¬ ê°œë…ê³¼ ì •ì˜
- ì¤‘ìš”í•œ ì›ë¦¬ë‚˜ ë²•ì¹™

ğŸ¯ **í•µì‹¬ í¬ì¸íŠ¸**  
- ê¼­ ê¸°ì–µí•´ì•¼ í•  ë‚´ìš©
- ì‹œí—˜ì— ë‚˜ì˜¬ ë§Œí•œ ì¤‘ìš” ì‚¬í•­

ğŸ’¡ **ì‹¤ìŠµ/ì˜ˆì‹œ**
- êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‚˜ ì˜ˆì‹œ
- ì‹¤ì œ ì ìš© ë°©ë²•

ğŸ“ **ì •ë¦¬**
- ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
- ë‹¤ìŒ í•™ìŠµê³¼ì˜ ì—°ê²°ì """
                        },
                        {
                            "role": "user",
                            "content": f"ë‹¤ìŒì€ ì„ ìƒë‹˜ì´ 1ì‹œê°„ ë™ì•ˆ ì§„í–‰í•œ ìˆ˜ì—… ë‚´ìš©ì…ë‹ˆë‹¤. í•™ìƒë“¤ì˜ í•™ìŠµì„ ìœ„í•´ ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{request.text}"
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": 500
                }
            )
        
        if response.status_code != 200:
            error_text = response.text
            logger.error(f"LLM API ì˜¤ë¥˜: {response.status_code} {error_text}")
            logger.error(f"ì‚¬ìš©ëœ API URL: {api_url}")
            logger.error(f"API í‚¤ íŒ¨í„´: {openai_api_key[:10]}...")
            raise HTTPException(status_code=500, detail=f"LLM API í˜¸ì¶œ ì‹¤íŒ¨ (HTTP {response.status_code}): {error_text[:100]}")
        
        data = response.json()
        summary = data.get("choices", [{}])[0].get("message", {}).get("content")
        
        if not summary:
            raise HTTPException(status_code=500, detail="ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        return LLMSummarizeResponse(summary=summary.strip())
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"LLM ìš”ì•½ ì˜¤ë¥˜: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        logger.error(f"API í‚¤ ì¡´ì¬: {bool(openai_api_key)}")
        logger.error(f"API í‚¤ íŒ¨í„´: {openai_api_key[:10] if openai_api_key else 'None'}...")
        raise HTTPException(status_code=500, detail=f"LLM ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/filter-text", response_model=TextFilterResponse)
async def filter_text_api(request: TextFilterRequest):
    """ìœ ì‚¬ë„ ê¸°ë°˜ í…ìŠ¤íŠ¸ í•„í„°ë§ API"""
    try:
        if not request.text or not request.text.strip():
            raise HTTPException(status_code=400, detail="í•„í„°ë§í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        result = filter_text_by_similarity_with_okt(
            request.text, 
            request.similarity_threshold, 
            request.min_sentence_length
        )
        
        return TextFilterResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ í•„í„°ë§ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="í…ìŠ¤íŠ¸ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"message": "ğŸš€ êµìœ¡ í…ìŠ¤íŠ¸ ìš”ì•½ API ì„œë²„ ì‹¤í–‰ ì¤‘", "status": "ok"}

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port) 